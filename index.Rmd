---
title: "Bayesian nonparametric models for zero-inflated count-compositional data using ensembles of regression trees"
author:
  - "<strong>Andr√© F. B. Menezes<strong> -- MU"
  - Prof. Andrew Parnell -- UCD
  - Dr. Keefe Murphy -- MU
date: "CSML, Lancaster Univerisity, February 05, 2026"
output:
  xaringan::moon_reader:
    css: ["default", "./config/sydney.css", "./config/sydney-fonts.css"]
    self_contained: FALSE
    mathjax: default
    nature:
      ratio: "16:9"
      highlightStyle: github
      highlightLines: TRUE
      countIncrementalSlides: FALSE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = "center",
                      dev = "svg", fig.width = 10, fig.height = 6)
library(RefManageR)
library(ggplot2)
library(cowplot)
library(xtable)
library(dplyr)
theme_set(
  theme_cowplot(font_size = 16, font_family = "Palatino") +
    background_grid() +
    theme(legend.position = "top")
)
options(digits = 4L)

BibOptions(check.entries = FALSE, bib.style = "authoryear", 
           cite.style = 'authoryear',
           style = "markdown",
           hyperlink = TRUE, dashed = TRUE, max.names = 3, longnamesfirst = FALSE)
bib <- ReadBib("./references.bib", check = FALSE)
data_pollen <- read.csv(file = file.path("./data", "rs11_pollen.csv"))
data_pollen <- dplyr::as_tibble(data_pollen)
sp <- c("Pinus.D", "Betula", "Gramineae", "Picea", "Quercus.D", "Alnus", "Cyperaceae",
        "Chenopodiaceae", "Artemisia", "Quercus.E", "Salix", "Juniperus", "Ericales",
        "Fagus", "Abies", "Olea", "Ulmus", "Corylus", "Ostrya", "Pinus.H", "Cedrus",
        "Carpinus", "Pistacia", "Castanea", "Larix", "Tilia", "Ephedra", "Phillyrea")
data_pollen$specie <- forcats::fct_relevel(data_pollen$specie, sp)
data_pollen

data_pollen_wide <- data_pollen |>
  dplyr::select(-c(category, prop)) |>
  dplyr::mutate(total = as.integer(total)) |>
  tidyr::pivot_wider(names_from = specie, values_from = total) |>
  dplyr::select(gdd5, mtco, aet.pet, dplyr::all_of(sp))

tab_emp <- cbind(
  avg = colMeans(data_pollen_wide[, -(1:3)]),
  di = apply(data_pollen_wide[, -(1:3)], 2, var)/colMeans(data_pollen_wide[, -(1:3)]))
range(tab_emp[, 2])
```

# Outline

- Introduction

- Zero-inflation in count-compositional data

- The ZANIM-BART and ZANIM-LN-BART models

- Simulated example

- Modern pollen-climate application

- Conclusions

---
# What is count-compositional data?

> Multivariate counts constrained by a random or fixed total.

- The data are usually collected in a matrix $\mathbf{Y} \in \mathbb{N}^{n\times d}_0$,
with $n$ samples and $d$ categories.

- Each row is a random vector $\mathbf{Y}_i = (Y_{i1}, \ldots, Y_{id})$
with non-negative integer values defined in the discrete simplex space:
$$\mathbb{S}_N^d = \left\{\mathbf{Y}_i \in \left(0, 1, \ldots, N_i\right)^d; \sum_{j=1}^d Y_{ij}=N_i\right\}.$$


- Examples include:
  - pollen counts observed in sediments dating from several species.
  - genomic sequencing data (microbiome, single-cell RNA).
  - total votes of each candidate for the Irish presidential election.

---
## Motivating example

- Modern pollen-climate data: $n=7832$ samples, $d=28$ pollen species, $p=3$ climate covariates, total counts $N_i \in \lbrack74, 1003\rbrack$ `r Citep(bib, c("Haslett2006", "SalterTownshend2012", "Parnell2015"))`.

```{r glimpse}
data_pollen |>
  dplyr::select(-c(category, prop)) |>
  dplyr::mutate(total = as.integer(total)) |>
  tidyr::pivot_wider(names_from = specie, values_from = total) |>
  dplyr::select(gdd5, mtco, aet.pet, dplyr::all_of(sp))
```

---
## Motivating example

```{r map-location, out.width="100%", fig.align="center"}
knitr::include_graphics("./figures/mapping_location_total_zero.png")
```

---
## Motivating example

```{r plot-pollen, echo = FALSE, out.width="90%"}
data_pollen |>
  dplyr::filter(specie %in% sp[1:9]) |>
  ggplot(aes(x = mtco, y = prop)) +
  facet_wrap(~specie) +
  geom_point(alpha = 0.3, size = 0.8) +
  geom_rug(data = dplyr::filter(data_pollen, specie %in% sp[1:9], total == 0),
           aes(y = NA_real_), col = "red", alpha = 0.5) +
  labs(y = "Pollen composition", x = "Mean temperature of coldest month (MTCO)")
```

---
## Key features of pollen-climate count-compositional data

<!-- - High-dimension: $d = 28$ pollen species. -->

- Sparse: $63.21\%$ of counts are zero.
  - $N$-inflated observations:
```{r n-inflated}
dplyr::select(dplyr::filter(data_pollen, prop == 1), id, specie, total, prop, 
              gdd5, mtco, aet.pet, longitude, latitude, altitude)
```

- Heterogeneity: $\operatorname{DI}(Y_j)=\operatorname{Var}(Y_j)/\mathbb{E}(Y_j)$ ranging from $41.15$ (_Ostrya_) and $470.18$ (_Cedrus_),
indicating overdispersion.

- Unknown relationship between the climate variables and the pollen composition.
  - Gaussian Markov random field (GMRF) have been used as a smooth prior function in one, two and three-dimensional climate space `r Citep(bib, c("Haslett2006", "SalterTownshend2012", "Sweeney2012", "Tipton2019"))`.

---
## Related work

> - **Palaeoclimate studies**: relax the compositional structure of the counts into
independent univariate distributions using expert-informed hierarchical category (pollen species) structures.
`r Citep(bib, c("SalterTownshend2012", "Sweeney2012"))`.
> - **Microbiome studies**: explicitly account for zero-inflation,
but assumed linear functional form to account for covariates:
`r Citep(bib, c("Tang2018", "Zeng2023", "Koslovsky2023"))`.

--

.font120[Current work]

> 1. A unified framework for zero-inflation in count-compositional data `r Citep(bib, "Menezes2025")`.
> 2. Bayesian nonparametric models based on ensembles of regression trees that
jointly address zero-inflation, overdispersion,
and accomodate complex covariates relationships.

---
count: false
class: middle, inverse
# Zero-inflation in count-compositional data

---
# Zero-inflation in count-compositional data

- Zeros may have different meanings `r Citep(bib, "BlascoMoreno2019")`:
  - Sampling zeros: due to sampling variability.
  - Structural zeros: related to constraints on experimental conditions (data collection). 

- The common probability distributions (multinomial and Dirichlet-multinomial)
for count-compositional can not handle excess of zeros.

--

- Excess zeros can occur in a single category or across multiple categories.
  
  - $d = 5$ and $N = 12$: $\mathbf{Y}_i = (2, 3, 0, 4, 3)$, $\mathbf{Y}_i = (9, 3, 0, 0, 0)$ or
$\mathbf{Y}_i = (12, 0, 0, 0, 0)$.

  - There are $2^d - 1$ different settings where zeros can occur in the random vector $\mathbf{Y}_i \in \mathbb{S}^d_{N_i}$.

  - In extreme cases where zeros co-occur in all but one category, the count for the
remaining category will coincide with the number of trials, $N_i$.

---
# The zero-and-N-inflated multinomial distribution

- $\mathbf{Y}_i \sim \operatorname{Multinomial}\left\lbrack N_i, \boldsymbol{\theta}\right\rbrack$,
where 
$\boldsymbol{\theta} = (\theta_1, \ldots, \theta_d)$ are the *population-level* count (compositional)
probabilities defined on
$\{\pmb{\theta}\in\mathbb{R}^d; \theta_j > 0, \sum_{j=1}^d \theta_j=1\}$.

--
- Reparameterise $\theta_j=\lambda_j/\sum_{k=1}^d \lambda_k$ and introduce the latent 
variable $(\phi_i \mid \boldsymbol{\lambda}, \mathbf{y}_i) \sim \operatorname{Gamma}\lbrack N_i, \sum_{j=1}^d\lambda_j\rbrack$,
such that the joint density of $\mathbf{Y}_i$ and $\phi_i$ is `r Citep(bib, "Baker1994")`:
$$p(\mathbf{y}_i,\phi_i;\boldsymbol{\lambda})=\dfrac{N_i!\phi_i^{N_i-1}}{\Gamma(N_i)}\prod_{j=1}^d\left\lbrack\dfrac{\lambda_j^{y_{ij}}e^{-\lambda_j\phi_i}}{y_{ij}!}\right\rbrack.$$

--
- We extend this to a zero-inflated Poisson form with a vector
$\boldsymbol{\zeta} = (\zeta_1, \ldots, \zeta_d)$ of  structural zero probabilities
for each category, such that $\zeta_j \in \lbrack 0, 1 \rbrack$, i.e.
$$p(\mathbf{y}_i,\phi_i; \boldsymbol{\lambda}, \boldsymbol{\zeta}) =
\dfrac{N_i!\,\phi_i^{N_i - 1}}{\Gamma(N_i)}\,\prod_{j=1}^d \left\lbrack
\zeta_j\,I_0(y_{ij}) + (1 - \zeta_j)\,\dfrac{\lambda_j^{y_{ij}}\,e^{-\lambda_j\,\phi_i}}{y_{ij}!}
\right\rbrack$$
and then integrate out $\phi_i$ to obtain the zero-and-_N_-inflated multinomial (ZANIM) distribution.

---
# ZANIM: finite mixture representation

$$\begin{align}\Pr\lbrack
\mathbf{Y}_i = \mathbf{y}_i; \pmb{\theta}, \pmb{\zeta}\rbrack &= \eta_d\,\binom{N}{y_{i1} \dots y_{id}}\,
\prod_{j=1}^d
\theta_j^{y_{ij}} \leftarrow \fbox{Multinomial component}\\
\fbox{All-inflation component}\rightarrow  &\phantom{=}~+\eta_0\,\prod_{j=1}^{d}\,I_0(y_{ij})\\
\fbox{N-inflation components}\rightarrow &\phantom{=}~+
\sum_{j=1}^{d}\,
\eta_{N}^{(j)}
\left\lbrack 
I_0\left(\sum_{k\colon k \neq j} y_{ik} \right)\,
\right\rbrack \\
\fbox{Reduced multinomials components}\rightarrow 
&\phantom{=}~+
\sum_{\mathcal{K} \in \mathfrak{K}} 
\eta_{\mathcal{K}}
\left\lbrack
I_0\left(\sum_{k \in \mathcal{K}} y_{ik}\right) 
\binom{N}{\{y_{ij}\}_{j \notin \mathcal{K}}} 
\prod_{j \notin \mathcal{K}} \left( \theta_j^{\mathcal{K}} \right)^{y_{ij}}
\right\rbrack  
\end{align}$$
for $\mathbf{y}_i \in \mathbb{S}^d_{N_i} \cup \mathbf{0}_d$,
where
$\theta_j^{\mathcal{K}} = \dfrac{\theta_j}{1 - \sum_{\ell \in \mathcal{K}}\,\theta_{\ell}}$,
and 
$\mathfrak{K} = \{\mathcal{K} \subseteq \{1,\ldots,d\}; 1 \leq \lvert\mathcal{K}\rvert \leq d-2\}$.

--
> We do not need to evaluate ALL $2^d$ mixture components and there are only 
$2d$ parameters, as the mixture weights $\boldsymbol{\eta}$ are simple functions of
the $\boldsymbol{\zeta}$ parameters.

---
## Stochastic representation

If $\mathbf{Y}_i \sim \operatorname{ZANIM}_d\lbrack N_i, \boldsymbol{\theta}, \boldsymbol{\zeta} \rbrack$,
then $\mathbf{Y}$ has the stochastic representation:
$$\begin{align*}
(z_{ij} \mid \zeta_{j}) & \overset{\operatorname{ind.}}{\sim} \operatorname{Bernoulli}\lbrack 1 - \zeta_j \rbrack, \quad j=1,\ldots, d\\
(\mathbf{Y}_i \mid N_i, \boldsymbol{\theta}, \boldsymbol{z}_i) &\sim
\begin{cases} \delta_{\mathbf{0}_d}(\cdot) & \textrm{if} \: z_{ij} = 0 \: \forall j\\
\operatorname{Multinomial}_d\left\lbrack N, \dfrac{z_{i1}\theta_1}{\sum_{k=1}^dz_{ik}\theta_k}, \ldots, \dfrac{z_{id}\theta_d}{\sum_{k=1}^dz_{ik}\theta_k}\right\rbrack
& \textrm{otherwise}.
\end{cases}
\end{align*}$$

> We refer to $$\vartheta_{ij} = \frac{z_{ij}\theta_j}{\sum_{k=1}^d z_{ik}\theta_k}$$
as the *individual-level* count probabilities, for $i\in\{1,\ldots,n\}$ and $j\in\{1,\ldots,d\}$.

---
### Marginal PMF $\pmb{\theta} = (0.05, 0.70, 0.25)$, $\pmb{\zeta} = (0.05, 0.15, 0.10)$, and $N=30$

```{r marginal-pmf, fig.width = 12, fig.height = 4}
d <- 3L
m <- 30L
theta <- c(0.05, 0.70, 0.25)
zeta <- c(0.10, 0.15, 0.05)

xx <- seq.int(0, m)
pmf_zanim <- matrix(data = 0L, nrow = length(xx), ncol = d)
pmf_mult <- matrix(data = 0L, nrow = length(xx), ncol = d)

for (i in seq_along(xx)) {
  pmf_zanim[i, 1L] <- zanim::dzanim_marginal(x = xx[i], size = m, prob = theta,
                                      zeta = zeta, j = 1L)
  pmf_zanim[i, 2L] <- zanim::dzanim_marginal(x = xx[i], size = m, prob = theta,
                                      zeta = zeta, j = 2L)
  pmf_zanim[i, 3L] <- zanim::dzanim_marginal(x = xx[i], size = m, prob = theta,
                                      zeta = zeta, j = 3L)
}
pmf_mult[, 1L] <- dbinom(x = xx, size = m, prob = theta[1L])
pmf_mult[, 2L] <- dbinom(x = xx, size = m, prob = theta[2L])
pmf_mult[, 3L] <- dbinom(x = xx, size = m, prob = theta[3L])


data_pmf_zanim <- dplyr::tibble(dist = "ZANIM",
                                x = rep(xx, d),
                                pmf = c(pmf_zanim),
                                cat = rep(paste0("j == ", 1:d),
                                          each = length(xx)))
data_pmf_mult <- dplyr::tibble(dist = "Multinomial",
                               x = rep(xx, d),
                               pmf = c(pmf_mult),
                               cat = rep(paste0("j == ", 1:d),
                                         each = length(xx)))
data_pmf <- rbind(data_pmf_mult, data_pmf_zanim)
ggplot(data = dplyr::filter(data_pmf),
            aes(x = x, y = pmf, col = dist)) +
  facet_wrap(~cat, nrow = 1L, scales = "free", labeller = label_parsed) +
  geom_pointrange(mapping = aes(ymin = 0, ymax = pmf),
                  size = 0.12, fill = 0.2) +
  scale_x_continuous(breaks = scales::pretty_breaks(8)) +
  labs(x = "k", y = latex2exp::TeX(r'($\Pr\lbrack Y_j = k \rbrack$)'),
       col = "") +
  colorspace::scale_color_discrete_qualitative()
```

--
> We also show that the zero-and-_N_-inflated Dirichlet-multinomial (ZANIDM) distribution
intoduced by `r Citet(bib, "Koslovsky2023")`
admits a finite mixture representation and we derive moments, marginals, and fully conjugate Bayesian inference schemes for both distributions.

---
# [Menezes, Parnell, and Murphy (2025)]()

```{r paper-jmva, out.width="50%"}
knitr::include_graphics("./figures/paper_jmva.png")
```

> Menezes, A.F., Parnell, A.C., Murphy, K., 2025.
Finite mixture representations of zero-and-_N_-inflated distributions for count-compositional data. Journal of Multivariate Analysis 210, 105492. doi:10.1016/j.jmva.6342025.105492.

---
count: false
class: middle, inverse
# The ZANIM-BART and ZANIM-LN-BART models

---
# Overview of the nonparametric BART prior

.font70[

- Bayesian additive regression trees (BART) is a nonparametric prior that represents 
an unknown function $f(\mathbf{x}_i)$ of interest as a sum of decision trees `r Citep(bib, "Chipman2010")`:
$$f(\mathbf{x}_i) = \sum_{h=1}^{m}g(\mathbf{x}_i, \mathcal{T}_h, \mathcal{M}_h),$$
where $g(\mathbf{x}_i, \mathcal{T}_h, \mathcal{M}_h)$ denotes a binary decision
tree parametrised by:
  - Binary tree topology $\mathcal{T}_h$ with terminal and internal nodes
  $\mathcal{L}_h$ and $\mathcal{B}_h$, respectively.
  - In each internal node $b\in \mathcal{B}_h$ there is a spitting rule of the form
$\lbrack x_{j_{b}} \leq c_b\rbrack$.
  - $\mathcal{M}_h = \{\mu_{ht}\colon t \in \mathcal{L}_h\}$ are terminal node parameters with $\mu_{ht} \in \mathbb{R}$.
]

--

.font70[

- The prior is $\pi(\mathcal{T}_h, \mathcal{M}_h) = \pi_\mathcal{T}(\mathcal{T}_h)\pi_{\mathcal{M}}(\mathcal{M}_h \mid \mathcal{T}_h)$.
  - $\pi_\mathcal{T}(\mathcal{T}_h)$ is a branching process `r Citep(bib, "Chipman1998")`.
  - $\pi_{\mathcal{M}}(\mathcal{M}_h \mid \mathcal{T}_h)=\prod_{t \in \mathcal{L}_h} \pi_\mu(\mu_{ht})$,
where $\pi_\mu$ is chosen so that it is conditionally conjugate.

- Inference on $\{(\mathcal{T}_h, \mathcal{M}_h)\}_{h=1}^m$
is conducted by MCMC using Bayesian backfitting `r Citep(bib, "Hastie2000")`.

]

--

.font70[

- The log-linear BART prior of `r Citet(bib, "Murray2021")` reparameterises the terminal node parameters via $\lambda_{ht} = e^{\mu_{ht}}$, where $\lambda_{ht}>0$, such that 
$\Lambda_h = \{\lambda_{ht}\colon t \in \mathcal{L}_h\}$. Then,
$$\log\left\lbrack f(\mathbf{x}_i) \right\rbrack = \sum_{h=1}^{m}\log\left\lbrack g\left(\mathbf{x}_i; \mathcal{T}_h, \Lambda_h\right)\right\rbrack 
= \sum_{h=1}^{m}\log(\lambda_{ht})1(\mathbf{x}_i \in \mathcal{A}_{ht}), \quad t \in 
\mathcal{L}_h.$$
]

---
# The BART prior

```{r trees-mateus, out.width="70%", fig.align="center"}
knitr::include_graphics("./figures/trees_from_mmm.png")
```


---
# The ZANIM-BART model

- log-linear BART prior of `r Citet(bib, "Murray2021")` for the compositional probabilities, $\pmb{\theta}$:

$$\theta_{ij} = \frac{f_j^{(\mathrm{c})}(\mathbf{x}_i)}{\sum_{k=1}^d f_k^{(\mathrm{c})}(\mathbf{x}_i)},
\quad 
\log f_j^{(\mathrm{c})}(\mathbf{x}_i) =
\sum_{h=1}^{m_\theta}\log\left\lbrack g\left(\mathbf{x}_i; \mathcal{T}^{(\mathrm{c})}_{hj},
\Lambda_{hj}\right)\right\rbrack,$$
where $\mathcal{T}^{(\mathrm{c})}_{hj}$ denotes the $h$-th binary tree topology
for the category $j$ and $\Lambda_{hj} = \{\lambda_{htj}\colon \mathcal{L}^{(\mathrm{c})}_{hj} \}$ 
are the corresponding set of terminal node parameters.

--

- probit-BART prior of `r Citet(bib, "Chipman2010")` for the structural zero probabilities, $\pmb{\zeta}$:
$$\zeta_{ij} = \Phi\left( f^{(0)}_j(\mathbf{x}_i) \right) = \Phi\left\lbrack \sum_{h=1}^{m_\zeta}  g\left(\mathbf{x}_i; \mathcal{T}^{(0)}_{hj}, \mathcal{M}_{hj}\right) \right\rbrack,$$
where $\Phi(\cdot)$ is standard normal cumulative distribution function,
and  $\mathcal{T}^{(0)}_{hj}$ and $\mathcal{M}_{hj} = \{\mu_{htj}\colon \mathcal{L}^{(0)}_{hj} \}$
are the category-specific tree structure and terminal node parameters, respectively. 

---
# Incoporating heterogeneity: The ZANIM-LN-BART model

.font80[
- Include a log-linear multivariate Gaussian random effects,
$\mathbf{u}_i = (u_{i1}, \ldots, u_{id}) \sim \operatorname{Normal}_d\left\lbrack \pmb{0}, \pmb{\Sigma}_U \right\rbrack$,
on the compositional probabilities, $\pmb{\theta}$, as follows:
$$\theta_{ij} = \frac{f_j^{(\mathrm{c})}(\mathbf{x}_i)e^{u_{ij}}}{\sum_{k=1}^d f_k^{(\mathrm{c})}(\mathbf{x}_i)e^{u_{ik}}}.$$
]

--

.font80[
- Identifiability is addressed via sum-to-zero constraint: $\mathbf{u}_i=\mathbf{B}\mathbf{v}_i$, $\mathbf{v}_i \sim \operatorname{Normal}_{d-1}\left\lbrack \pmb{0}, \pmb{\Sigma}_V \right\rbrack$, where $\mathbf{B}$ is $d\times d-1$ orthogonal matrix, and $\pmb{\Sigma}_U = \mathbf{B} \pmb{\Sigma}_V \mathbf{B}^\top$.
- Sample from $\mathbf{v}_i$ using elliptical slice sampling.
- Factor analysis hyper-prior on $\pmb{\Sigma}_V$.

$$\mathbf{v}_i = \pmb{\Gamma} \pmb{\eta}_i + \pmb{\epsilon}_i,$$
where $\pmb{\Gamma} = \left\lbrack \gamma_{hj}\right\rbrack_{h=1,\ldots,d-1}^{j=1,\ldots,k}$ is a
$d-1 \times k$ factor loading matrix, 
$\pmb{\eta}_i \sim \operatorname{Normal}_k\lbrack \mathbf{0}_k, \mathbf{I}_k\rbrack$ and
$\pmb{\epsilon}_i \sim \operatorname{Normal}_{d-1}\left\lbrack\mathbf{0}_{d-1}, \pmb{\Psi}\right\rbrack$, with
$\pmb{\Psi}=\operatorname{diag}\left\{ \psi_1, \ldots, \psi_{d-1}\right\}$.

- We shrink the contributions of the redundant loadings columns using the
multiplicative gamma process shrinkage prior of `r Citet(bib, "Bhattacharya2011")`.
]

---
# Sthocastic representation


$$\begin{align}
\left(z_{ij} \mid f_j^{(0)}(\mathbf{x}_i) \right) &\overset{\operatorname{ind.}}{\sim}
\operatorname{Bernoulli}\left\lbrack 1 - \Phi\left( f_j^{(0)}(\mathbf{x}_i) \right) \right\rbrack, \nonumber\\ \nonumber
\mathbf{v}_i & \sim \operatorname{Normal}_{d-1}\left\lbrack \mathbf{0}_{d-1}, \pmb{\Sigma}_V\right\rbrack, \\
\vartheta_{ij} &= \frac{z_{ij}f_j^{(\mathrm{c})}(\mathbf{x}_i)e^{u_{ij}}}{\sum_{k=1}^d z_{ij}f_k^{(\mathrm{c})}(\mathbf{x}_i)e^{u_{ik}}}
,\\ \nonumber
(\mathbf{Y}_i \mid N_i, \pmb{\vartheta}_{i}) &\sim
\begin{cases} \delta_{\mathbf{0}_d}(\cdot), & \textrm{if} \: z_{ij} = 0 \: \forall j\in\{1,\ldots, d\}, \nonumber\\
\operatorname{Multinomial}_d\left\lbrack N_i, \vartheta_{i1}, \ldots, \vartheta_{id}\right\rbrack,
& \textrm{otherwise},
\end{cases}
\end{align}$$
where $\mathbf{u}_i=\mathbf{B}\mathbf{v}_i$.

- Setting $\mathbf{u}_i=\mathbf{0}_d$ leads to the ZANIM-BART model.

---
# Likelihood

Let $\smash{\pmb{f}^{(\mathrm{c})} = (f_1^{(\mathrm{c})}, \ldots, f_d^{(\mathrm{c})})}$ and $\smash{\pmb{f}^{(0)} = (f_1^{(0)}, \ldots, f_d^{(0)})}$. We use two steps of data augmentation:
$$\left(\phi_i \mid \mathbf{y}_i, \mathbf{z}_i, \mathbf{u}_i, \pmb{f}^{(\mathrm{c})} \right) \overset{\operatorname{ind.}}{\sim}
\operatorname{Gamma}\left\lbrack N_i, \sum_{j=1}^d z_{ij}f_j^{(\mathrm{c})}(\mathbf{x}_i)e^{u_{ij}}\right\rbrack,$$
and
$$(w_{ij} \mid z_{ij}, f_j^{(0)}) \sim 
\begin{cases} 
w_{ij} \sim \operatorname{TN}_{\lbrack -\infty, 0\rbrack}\lbrack f_j^{(0)}(\mathbf{x}_i), 1 \rbrack & \textrm{if} \: z_{ij} = 1\\
w_{ij} \sim \operatorname{TN}_{\lbrack 0, \infty\rbrack}\lbrack f_j^{(0)}(\mathbf{x}_i), 1 \rbrack & \textrm{if} \: z_{ij} = 0.
\end{cases}$$
Then, the augmented likelihood is
$$\mathscr{L}\left(\pmb{f}^{(\mathrm{c})}, \pmb{f}^{(0)}; \mathbf{y}, \mathbf{x}, \mathbf{u}, \mathbf{z}, \mathbf{w}, \pmb{\phi}\right)
\propto
\prod_{i=1}^n
\prod_{j=1}^d
\left\{
\varphi\left(w_{ij}; f^{(0)}_j(\mathbf{x}_i), 1\right)
\left\lbrack f_j^{(\mathrm{c})}(\mathbf{x}_i)\right\rbrack^{y_{ij}}
e^{-\phi_i z_{ij}e^{u_{ij}}f_j^{(\mathrm{c})}(\mathbf{x}_i)}
\right\},$$
where $\varphi(x; \mu, \sigma^2)$ is the Gaussian probability density function 
with mean $\mu$ and variance $\sigma^2$.

---
# Priors

- For the priors on the tree topologies $\smash{\mathcal{T}^{(\mathrm{c})}_{hj}}$ and $\smash{\mathcal{T}^{(0)}_{hj}}$, we adopt the branching process prior introduced by `r Citet(bib, "Chipman1998")`.

- Conditionally conjugate priors for $\{\Lambda_{hj}\}_{h=1}^{m_\theta}$ and $\{\mathcal{M}_{hj}\}_{h=1}^{m_\zeta}$:
  - $\smash{\lambda_{htj} \overset{\operatorname{ind.}}{\sim} \operatorname{Gamma}\lbrack c_0, d_0\rbrack}.$
  - $\mu_{htj} \overset{\operatorname{ind.}}{\sim} \operatorname{Normal}\lbrack 0, \sigma^2_\mu\rbrack.$

- The hyperparameter $\sigma^2_\mu$ is calibrated following `r Citet(bib, "Chipman2010")`:
  - $\sigma_\mu = 0.5 / (k\sqrt{m_\zeta})$, where $k=2$ is such that $\smash{f_j^{(0)}}$
has high prior probability of lying in $(-3.0, 3.0)$

- The hyperparameters $c_0$ and $d_0$ are calibrated following `r Citet(bib, "Murray2021")`:
  - $\mathbb{E}\lbrack \lambda_{htj} \rbrack = 0$ and  $\operatorname{Var}\lbrack \lambda_{htj}\rbrack = a^2_\lambda / m_\theta$, where $a_\lambda$ is a tuning parameter.
  - We place hyperprior $a_\lambda \sim \operatorname{half-Cauchy}\lbrack 0, 1\rbrack$ and updating $a_\lambda$ using slice sampling.

---
# Posterior inference, $(\mathcal{T}^{(c)}_{hj}, \Lambda_{hj})_{h=1}^{m_\theta}$

- Let $\smash{f^{(\mathrm{c})}_{(h)j}}(\mathbf{x}_i) = \prod_{\ell \neq h} g(\mathbf{x}_i; \mathcal{T}^{(\mathrm{c})}_{\ell j}, \Lambda^{(\mathrm{c})}_{\ell j})$.

- The integrated likelihood of $\mathcal{T}^{(\mathrm{c})}_{hj}$ is

$$\pi\left(\mathcal{T}^{(\mathrm{c})}_{hj} \mid \mathcal{T}^{(\mathrm{c})}_{(h)j}, \Lambda_{(h)j}, \mathbf{y},
\mathbf{z}, \mathbf{u}, \pmb{\phi}\right) 
\propto
\pi_{\mathcal{T}}\left(\mathcal{T}^{(\mathrm{c})}_{hj}\right)
\prod_{t \in \mathcal{L}^{(\mathrm{c})}_{hj}}
\frac{d_0^{c_0}}{\Gamma(c_0)}
\frac{\Gamma\left(r^{(\mathrm{c})}_{htj} + c_0\right)}{(s^{(\mathrm{c})}_{htj} + d_0)^{r^{(\mathrm{c})}_{htj}+c_0}},$$

- The posterior distribution of ${\Lambda_{hj}}$ is

$$\left(
\lambda_{thj} \mid 
\mathcal{T}^{(\mathrm{c})}_{hj},  \mathcal{T}^{(\mathrm{c})}_{(h)j}, \Lambda_{(h)j}, \mathbf{y}, \mathbf{z}, \pmb{\phi}
\right)
\overset{\operatorname{ind.}}{\sim}\operatorname{Gamma}\left\lbrack r^{(\mathrm{c})}_{htj} + c_0, s^{(\mathrm{c})}_{htj} + d_0\right\rbrack,
\quad t \in  \mathcal{L}^{(\mathrm{c})}_{hj}.$$
where $\smash{r^{(\mathrm{c})}_{htj}}=\sum_{i\colon\mathbf{x}_i \in \mathcal{A}^{(\mathrm{c})}_{htj}} y_{ij}$
and $\smash{s^{(\mathrm{c})}_{htj}}=\sum_{i\colon\mathbf{x}_i \in \mathcal{A}^{(\mathrm{c})}_{htj}}\phi_i z_{ij} e^{u_{ij}} f^{(\mathrm{c})}_{(h)j}(\mathbf{x}_i)$.

---
# Posterior inference, $(\mathcal{T}^{(0)}_{hj}, \mathcal{M}_{hj})_{h=1}^{m_\zeta}$

- Let $r_{(h)ij} \equiv w_{ij} - \sum_{\ell \neq h}\smash{g(\mathbf{x}_i; \mathcal{T}^{(0)}_{\ell j}, \mathcal{M}_{\ell j})}$.

- The integrated likelihood of $\mathcal{T}^{(0)}_{hj}$ is

$$\pi\left(\mathcal{T}^{(0)}_{hj} \mid \mathbf{r}_{(h)j}\right)
\propto 
\pi_{\mathcal{T}}\left(\mathcal{T}^{(0)}_{h}\right)
\prod_{t \in \mathcal{L}^{(0)}_{hj}}
\left(\dfrac{1}{n_{htj}\sigma_\mu^2 + 1}\right)^{1/2}\!
\exp\left\lbrack \dfrac{1}{2} \left(  \dfrac{\sigma^2_\mu\left(s^{(0)}_{htj}\right)^2}{2(n_{htj}\sigma^2_\mu + 1)} \right)\right\rbrack,,$$

- The posterior distribution of ${\mathcal{M}_{hj}}$ is
$$\left(\mu_{htj} \mid \mathcal{T}^{(0)}_{hj}, \mathbf{r}_{(h)j} \right) \overset{\operatorname{ind.}}{\sim} \operatorname{Normal}\left\lbrack 
\dfrac{s^{(0)}_{htj}}{n_{htj} + \sigma^2_\mu}, 1 / (n_{htj} + 1 / \sigma_\mu^2)\right\rbrack, \quad t \in \mathcal{L}^{(0)}_{hj}.$$
where 
$\smash{s^{(0)}_{htj}} = \sum_{i\colon \mathbf{x}_i \in \mathcal{A}^{(0)}_{htj}}r_{(h)ij}$
and $n_{htj}$ is the total number of observations in the partition $\smash{\mathcal{A}^{(0)}_{htj}}$.

---
# MCMC algorithm
```{r mcmc-algorithm, out.width="80%"}
knitr::include_graphics(path = "./figures/one_iter_algorithm.png")
```


---
count: false
class: middle, inverse
# Simulated example

---
# Simulated example

- Settings: $d = 4$, $n = 400$, $x_i \in \lbrack -1, 1\rbrack$,
$N_i \sim \operatorname{Uniform}\lbrack 100, 500\rbrack$.

$$\mathbf{Y}_i \sim \operatorname{Multinomial}\left\lbrack N_i, 
\frac{z_{i1}\theta_{i1}}{\sum_{k=1}^4z_{ik}\theta_{ik}}, \ldots, \frac{z_{i4}\theta_{i4}}{\sum_{k=1}^4z_{ik}\theta_{ik}} \right\rbrack, \quad i\in\{1,\ldots,400\},$$
where $z_{ij} \sim \operatorname{Bernoulli}\lbrack 1 - \zeta_{ij}\rbrack$.

- Functional form of the parameters are given by
$$\theta_{ij} = \frac{f_j^{(c)}(x_i)}{\sum_{k=1}^d f^{(c)}_j(x_i)},
\quad \textrm{with} \quad
\log f^{(j)}(x_i) = \sum_{\ell=1}^{b_\lambda} s^{\lambda}_{j\ell}(x_i) \beta_{j\ell},
\quad \textrm{and} \quad
\zeta_{ij} = \Phi\left( \sin(2 \pi x_i) + x_i^2 - \beta_{j0} \right)$$
where $s^{\lambda}_{j\ell}(\cdot)$
is basis of cubic B-splines with six degrees,
$\beta_{j\ell}$ were sampled from a standard normal distribution, and
$\beta_{j0} = (0.5, 1.0, 1.5, 2.0)$.

- We compared ZANIM-BART against multinomial-BART model of `r Citet(bib, "Murray2021")`,
and ZANIDM linear regression model of `r Citet(bib, "Koslovsky2023")`.



---
# Simulated example
```{r toy-example}
n_sample <- 400L
n_trials <- sample(seq.int(100L, 500L), n_sample, replace = TRUE)
set.seed(1212)
d <- 4L
dof_bs_theta <- 6L
X <- as.matrix(seq(-1, 1, length.out = n_sample))
X1_bs <- splines::bs(X, dof_bs_theta)
betas_theta <- matrix(stats::rnorm(d * dof_bs_theta), dof_bs_theta, d)
betas_theta[1L, ] <- betas_theta[1L, ] - seq(from = 4, to = 0, length.out = d)
eta_theta <- X1_bs %*% betas_theta
eta_zeta <- matrix(nrow = n_sample, ncol = d)
intercept <- c(0.5, 1.0, 1.5, 2.0)
for (j in seq_len(d)) {
  eta_zeta[, j] <- sin(2 * pi * X[, 1L]) + X[, 1L]^2 - intercept[j]
}
true_zetas <- pnorm(eta_zeta)
alphas <- exp(eta_theta) 
Y <- Z <- true_thetas <- true_varthetas <- matrix(nrow = n_sample, ncol = d)
for (i in seq_len(n_sample)) {
  z <- stats::rbinom(n = d, size = 1L, prob = 1.0 - true_zetas[i, ])
  is_zero <- z == 0L
  p_ij <- alphas[i, ] / sum(alphas[i, ])
  true_thetas[i, ] <- p_ij
  true_varthetas[i, ] <- z * p_ij / sum(z * p_ij)
  if (all(is_zero)) Y[i, ] <- rep(0L, d)
  else if (sum(is_zero) == d - 1L) {
    Y[i, ] <- rep(0L, d)
    Y[i, !is_zero] <- n_trials[i]
  } else {
    Y[i, ] <- stats::rmultinom(n = 1L, size = n_trials[i],
                               prob = true_varthetas[i, ])
  }
  Z[i, ] <- z
}
data_sim <- data.frame(id = rep(seq_len(n_sample), each = d),
                      category = rep(seq_len(d), times = n_sample),
                      x = rep(X[, 1L], each = d),
                      theta = c(t(true_thetas)),
                      zeta = c(t(true_zetas)),
                      total = c(t(Y)), z = c(t(Z)),
                      prop = c(apply(Y, 1L, function(z) z/sum(z))))
data_sim$category_lab <- paste0("j == ", data_sim$category)
data_sim$prop[which(is.na(data_sim$prop))] <- 0.0
ggplot(data_sim, aes(x = x, y = prop)) +
  facet_wrap(~category_lab, labeller = label_parsed) +
  geom_point(alpha = 0.5, aes(col = "y")) +
  geom_line(aes(y = theta, col = "theta")) +
  geom_line(aes(y = zeta, col = "zeta")) +
  labs(y = latex2exp::TeX(r'(Composition, $y_{ij}/N_i$)'),
       x = expression(x[i]), col = "", fill = "") +
  scale_color_manual(
    breaks = c("y", "theta", "zeta"),
    values = c("y" = "black", "theta" = "blue", "zeta" = "red"),
    labels = c("y" = latex2exp::TeX(r'($y_{ij}/N_i$)'),
               "theta"  = latex2exp::TeX(r'($\theta_{ij}$)'),
               "zeta"   = latex2exp::TeX(r'($\zeta_{ij}$)')))
# Import data with the results
tmp <- readRDS(file = "./data/posterior_parameters.rds")
data_theta <- tmp[[1L]]
data_zeta <- tmp[[2L]]
COLORS <- colorspace::qualitative_hcl(3, palette = "Dark 3")
```


---
# Inference for compositional probabilities

```{r count-prob}
ggplot(data = data_sim) +
  geom_line(mapping = aes(x = x, y = theta), linewidth = 0.8) +
  facet_wrap(~category_lab, labeller = label_parsed) +
  geom_rug(data = dplyr::filter(data_sim, total == 0L),
           mapping = aes(y = NA_real_, x = x)) +
  geom_line(data = data_theta, mapping = aes(x = x, y = median, col = model)) +
  geom_ribbon(data = data_theta,
              aes(x = x, ymin = ci_lower, ymax = ci_upper, fill = model),
              alpha = 0.3) +
  labs(y = latex2exp::TeX(r'(Count probabilities, $\theta_{ij}$)'),
       x = expression(x[i]), col = "", fill = "") +
  scale_color_manual(values = COLORS) +
  scale_fill_manual(values = COLORS) 
```


---
# Inference for structural zero probabilities

```{r zero-prob}
ggplot(data = data_sim) +
  geom_line(mapping = aes(x = x, y = zeta), linewidth = 0.8) +
  facet_wrap(~category_lab, labeller = label_parsed) +
  geom_rug(data = dplyr::filter(data_sim, total == 0L),
           mapping = aes(y = NA_real_, x = x)) +
  geom_line(data = data_zeta, mapping = aes(x = x, y = median, col = model)) +
  geom_ribbon(data = data_zeta, aes(x = x, ymin = ci_lower, ymax = ci_upper,
                                    fill = model), alpha = 0.3) +
  labs(y = latex2exp::TeX(r'(Zero-inflation probabilities, $\zeta_{ij}$)'),
       x = expression(x[i]), col = "", fill = "") +
  scale_color_manual(values = COLORS[-1]) +
  scale_fill_manual(values = COLORS[-1]) 
```


---
count: false
class: middle, inverse
# Modern pollen-climate application


---
# Context

- $n=7832$ samples of $d=28$ pollen species collected at different site locations in the 
Northern Hemisphere, along with three climate covariates `r Citep(bib, "Parnell2015")`: 
  - growing degree days above five $5^\circ$ (GDD5): growing season warmth.
  - mean temperature of the coldest month (MTCO): harshness of the winter.
  - ratio of actual to potential evapotranspiration (AET/PET): available moisture.

- **Goal:** to investigate the climate-pollen relationship in a sparse (63% zeros) and highly heterogeneous dataset.

```{r plot-pollen-again, echo = FALSE, out.width="80%", eval=FALSE}
data_pollen |>
  dplyr::filter(specie %in% c("Pinus.D", "Picea")) |>
  ggplot(aes(x = mtco, y = prop)) +
  facet_wrap(~specie) +
  geom_point(alpha = 0.3, size = 0.8) +
  geom_rug(data = dplyr::filter(data_pollen, specie %in% c("Pinus.D", "Picea"),
                                total == 0),
           aes(y = NA_real_), col = "red", alpha = 0.5) +
  labs(y = "Pollen composition", x = "Mean temperature of coldest month (MTCO)")
```

- $m_\theta = m_\zeta = 200$ trees for $j\in\{1,\ldots, 28\}$. It took approximately $8$ hours for $10,000$ iterations.

---
# Goodness-of-fit

- Posterior-predictive distribution for the $2,000$ test samples.

```{r gof-pollen, out.width="70%"}
knitr::include_graphics("./figures/ppc_ecdf_paleoclimate_example.png")
```


---
# Posterior dependence plots

```{r}
knitr::include_graphics("./figures/pdps_pice_pinusd_2x3_fix.png")
```

---
## Posterior mean predictions given the observed climate: Pinus.D

```{r gdd5-mtco-pinus-d, out.width="100%"}
knitr::include_graphics("./figures/predictions_gdd5_mtco_pinus.d.png")
```

---
## Posterior mean predictions given the observed climate: Picea

```{r gdd5-mtco-picea, out.width="100%"}
knitr::include_graphics("./figures/predictions_gdd5_mtco_picea.png")
```


---
# Final remarks


.font80[

**Summary**

  - Probability characterisation for structural zero in count-composition data `r Citep(bib, "Menezes2025")`.

  - ZANIM-BART and ZANIM-LN-BART models address key challenges in 
count-compositional data: zero-inflation, overdispersion, cross-sample heterogeneity, 
unknown covariates effects.

  - Developed MCMC algorithm for ZANIM-BART and ZANIM-LN-BART models combining
  ZANIM data augmentation established BART sampling routines.
  
  - Illustrated the useful of our models in the analysis of modern pollen-climate data.
  
  - The models are implemented in the `R` package `zanicc` available in 
<a href="http://github.com/AndrMenezes/zanicc">`r icons::fontawesome("github")` AndrMenezes/zanicc</a>.
]

--

.font80[

**Future work**
  - Integrate our proposed models for the inverse prediction problem `r Citet(bib, "Parnell2015")`:
    Given new pollen counts $\mathbf{Y}_i^\ast$ would is the posterior of $\mathbf{X}^{\ast}$, i.e., $p(\mathbf{x}^{\ast} \mid \mathbf{Y}_i^\ast, \mathbf{Y}, \mathbf{x})$?.
  
  - Relax the independence across $\mathbf{z}_i$ using the
  probit seemingly unrelated BART (probit-suBART) prior of `r Citet(bib, "Esser2025")`.
]


---
count: false
# References

.font50[
```{r bib, results='asis', echo=FALSE}
RefManageR::PrintBibliography(bib, start = 1, end = 17)
```
]

---
count: false
class: middle, inverse
# Thank you!

.pull-right[

<a href="mailto:andrefelipemaringa@gmail.com">
`r icons::fontawesome("paper-plane")` andrefelipemaringa@gmail.com
</a>

<a href="https://andrmenezes.github.io/casi25">
`r icons::fontawesome("link")` andrmenezes.github.io/lancaster_csml__2026-02-05
</a>

<a href="http://github.com/AndrMenezes/zanicc">
`r icons::fontawesome("github")` AndrMenezes/zanicc
</a>

<br><br><br><br><br>
]

